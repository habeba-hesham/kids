# -*- coding: utf-8 -*-
"""tiny-llama-1b-kid-friendly-chatbot-tiny.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//huggingface.co/shashankverma590/tiny-llama-1b-kid-friendly-chatbot-tiny.ipynb
"""


"""## Local Inference on GPU
Model page: https://huggingface.co/shashankverma590/tiny-llama-1b-kid-friendly-chatbot-tiny

âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/shashankverma590/tiny-llama-1b-kid-friendly-chatbot-tiny)
			and/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ğŸ™
"""

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="shashankverma590/tiny-llama-1b-kid-friendly-chatbot-tiny")
messages = [
    {"role": "user", "content": "Who are you?"},
]
pipe(messages)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("shashankverma590/tiny-llama-1b-kid-friendly-chatbot-tiny")
model = AutoModelForCausalLM.from_pretrained("shashankverma590/tiny-llama-1b-kid-friendly-chatbot-tiny")
messages = [
    {"role": "user", "content": "Who are you?"},
]
inputs = tokenizer.apply_chat_template(
	messages,
	add_generation_prompt=True,
	tokenize=True,
	return_dict=True,
	return_tensors="pt",
).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=40)
print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))

user_message = "Hello! Who are you?"

input_ids = tokenizer.encode(user_message + tokenizer.eos_token, return_tensors="pt")

outputs = model.generate(input_ids, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)

reply = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
print("Bot reply:", reply)

import streamlit as st

if "messages" not in st.session_state:
    st.session_state.messages = []

st.title("ğŸ¤– Kid-Friendly Chatbot")

# Ø¹Ø±Ø¶ ÙƒÙ„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
for msg in st.session_state.messages:
    if msg["role"] == "user":
        st.markdown(f"**You:** {msg['content']}")
    else:
        st.markdown(f"ğŸ¤–: {msg['content']}")

# Ø¥Ø¯Ø®Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©
user_input = st.text_input("Type your message...")

if user_input:
    # Ø­ÙØ¸ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    st.session_state.messages.append({"role": "user", "content": user_input})

    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
    input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors="pt")
    outputs = model.generate(input_ids, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)
    reply = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)

    # Ø­ÙØ¸ Ø§Ù„Ø±Ø¯
    st.session_state.messages.append({"role": "bot", "content": reply})

    st.rerun()

